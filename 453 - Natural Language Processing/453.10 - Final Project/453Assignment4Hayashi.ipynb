{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 453 Assignment 4 - Chat Bot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General\n",
    "import os\n",
    "import pandas as pd\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "\n",
    "# NLP\n",
    "import nltk\n",
    "\n",
    "# Preprocessing\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize \n",
    "\n",
    "# Knowledge Graphs\n",
    "import spacy\n",
    "import networkx as nx\n",
    "\n",
    "# Model 1 - TF-IDF \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Model 2 - Sentence Transformers\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Model 3 - Huggin Face\n",
    "from transformers import pipeline\n",
    "\n",
    "# Model 4 - Llama Index\n",
    "from llama_index import VectorStoreIndex, SimpleDirectoryReader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_excel('vcs_conversations.xlsx',sheet_name='Caller')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Wrangling\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "def nlp_transformations(df):\n",
    "    \"\"\"\n",
    "    Apply NLP transformations to a pandas DataFrame.\n",
    "    Parameters: df (DataFrame): Input dataframe with a column named 'Text'.\n",
    "    Returns: DataFrame: A new dataframe with added columns for each NLP transformation.\n",
    "    \"\"\"\n",
    "    df['Tokens'] = df['Text'].apply(word_tokenize)\n",
    "    df['Normalized'] = df['Tokens'].apply(lambda x: [word.lower() for word in x])\n",
    "    df['Lemmatized'] = df['Normalized'].apply(lambda x: [lemmatizer.lemmatize(word) for word in x])\n",
    "    df['No_Stop_Words'] = df['Lemmatized'].apply(lambda x: [word for word in x if word not in stop_words])\n",
    "    df['Cleaned'] = df['No_Stop_Words'].apply(lambda x: [re.sub(r'[^a-zA-Z]', '', word) for word in x])\n",
    "    df['Cleaned'] = df['Cleaned'].apply(lambda x: [word for word in x if word != ''])\n",
    "    df['Cleaned'] = df['Cleaned'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "    return df\n",
    "\n",
    "wrangled_data = nlp_transformations(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrangled_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversations = data.loc[:,'Text'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Knowledge Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming df is your DataFrame and the column with conversations is named 'conversation'\n",
    "nlp = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.Graph()\n",
    "\n",
    "# Process each conversation\n",
    "for conversation in data['Text']:\n",
    "    doc = nlp(conversation)\n",
    "    for ent in doc.ents:\n",
    "        # Add entities as nodes\n",
    "        G.add_node(ent.text, type=ent.label_)\n",
    "\n",
    "    # Add possible relationships (edges)\n",
    "    for token in doc:\n",
    "        if token.dep_ in ('nsubj', 'dobj'):\n",
    "            subj = [w for w in token.head.lefts if w.dep_ == 'nsubj']\n",
    "            if subj:\n",
    "                G.add_edge(subj[0].text, token.text)\n",
    "\n",
    "# Use a spring layout to spread out the nodes\n",
    "pos = nx.spring_layout(G, scale=2)  # Scale parameter spreads nodes further apart\n",
    "\n",
    "# Specify the figure size\n",
    "plt.figure(figsize=(12, 12))  # You can adjust these dimensions as needed\n",
    "\n",
    "# Draw the graph with the specified layout and adjusted node and font sizes\n",
    "nx.draw(G, pos, with_labels=True, node_size=500, font_size=8, node_color=\"skyblue\", font_weight=\"bold\")\n",
    "\n",
    "# Show the graph\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modeling - Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_one = 'What departments do you have?'\n",
    "q_two = 'What are the store hours?'\n",
    "q_three = 'What sections does the general department have?'\n",
    "q_four = 'What services does the pharmacy offer?'\n",
    "q_five = 'What services does the photo department have?'\n",
    "q_six = 'How do callers feel about the pharmacy department?'\n",
    "q_seven = 'How do callers feel about the photo department?'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 1 - TF-IDF Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = vectorizer.fit_transform(conversations)\n",
    "\n",
    "def find_response(input_text):\n",
    "    # Transform the input text to tf-idf vector\n",
    "    input_tfidf = vectorizer.transform([input_text])\n",
    "    \n",
    "    # Calculate cosine similarity\n",
    "    cosine_similarities = cosine_similarity(input_tfidf, tfidf_matrix)\n",
    "    \n",
    "    # Find the most similar conversation\n",
    "    most_relevant = cosine_similarities.argsort()[0][-1]\n",
    "    return conversations[most_relevant]\n",
    "\n",
    "print(f'Question: {q_one}')\n",
    "print(f\"Answer: {find_response(q_one)[0:2]}\")\n",
    "print(f'Question: {q_two}')\n",
    "print(f\"Answer: {find_response(q_two)[0:2]}\")\n",
    "print(f'Question: {q_three}')\n",
    "print(f\"Answer: {find_response(q_three)[0:2]}\")\n",
    "print(f'Question: {q_four}')\n",
    "print(f\"Answer: {find_response(q_four)[0:2]}\")\n",
    "print(f'Question: {q_five}')\n",
    "print(f\"Answer: {find_response(q_five)[0:2]}\")\n",
    "print(f'Question: {q_six}')\n",
    "print(f\"Answer: {find_response(q_six)[0:2]}\")\n",
    "print(f'Question: {q_seven}')\n",
    "print(f\"Answer: {find_response(q_seven)[0:2]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 2 - Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Sentence Transformer model optimized for sentence cosine similarity calculations\n",
    "model = SentenceTransformer('multi-qa-MiniLM-L6-cos-v1')\n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('omw-1.4', quiet=True)\n",
    "\n",
    "# Read in data\n",
    "CORPUS_PATH = '/Users/dylanhayashi/Desktop/Northwestern/NU_MSDS/453 - Natural Language Processing/453.10 - Final Project/employee/employee.txt'\n",
    "with open(CORPUS_PATH, 'r', errors='ignore') as f:\n",
    "    raw = f.read().lower()  # Converts to lowercase\n",
    "\n",
    "# Create list of sentences and words\n",
    "sent_tokens = nltk.sent_tokenize(raw)  # Converts to list of sentences\n",
    "word_tokens = nltk.word_tokenize(raw)  # Converts to list of words\n",
    "\n",
    "# Greetings inputs and responses\n",
    "GREETING_INPUTS = (\"hello\", \"hi\", \"greetings\", \"sup\", \"what's up\", \"hey\")\n",
    "GREETING_RESPONSES = [\"Hello\"]\n",
    "\n",
    "def greeting(sentence):\n",
    "    \"\"\"If user's input is a greeting, return a greeting response\"\"\"\n",
    "    for word in sentence.split():\n",
    "        if word.lower() in GREETING_INPUTS:\n",
    "            return random.choice(GREETING_RESPONSES)\n",
    "\n",
    "def response(user_response):\n",
    "    \"\"\"Generate response to user input\"\"\"\n",
    "    chatbot_response = ''\n",
    "    sentence_encodings = model.encode(sent_tokens, convert_to_tensor=True)\n",
    "    sentence_encodings = sentence_encodings.cpu()\n",
    "    vals = cosine_similarity(sentence_encodings[-1].reshape(1, -1), sentence_encodings)\n",
    "    idx = vals.argsort()[0][-2]\n",
    "    flat = vals.flatten()\n",
    "    flat.sort()\n",
    "    second_cos_sim_val = flat[-2]\n",
    "    if second_cos_sim_val == 0:\n",
    "        chatbot_response = \"Sorry, I do not have an answer to your question in my database\"\n",
    "    else:\n",
    "        chatbot_response = sent_tokens[idx]\n",
    "    return chatbot_response\n",
    "\n",
    "def chatbot_response(user_input):\n",
    "    \"\"\"Process user input and return chatbot response\"\"\"\n",
    "    user_input = user_input.lower()\n",
    "    response_text = \"\"\n",
    "    if user_input != 'exit':\n",
    "        if user_input in ('thanks', 'thank you'):\n",
    "            response_text = \"You are welcome!\"\n",
    "        else:\n",
    "            if greeting(user_input) is not None:\n",
    "                response_text = greeting(user_input)\n",
    "            else:\n",
    "                sent_tokens.append(user_input)\n",
    "                word_tokens.extend(nltk.word_tokenize(user_input))\n",
    "                response_text = response(user_input)\n",
    "                sent_tokens.remove(user_input)\n",
    "    return response_text\n",
    "\n",
    "print(f'Question: {q_one}')\n",
    "print(f'Answer: {chatbot_response(q_one)}')\n",
    "print(f'Question: {q_two}')\n",
    "print(f'Answer: {chatbot_response(q_two)}')\n",
    "print(f'Question: {q_three}')\n",
    "print(f'Answer: {chatbot_response(q_three)}')\n",
    "print(f'Question: {q_four}')\n",
    "print(f'Answer: {chatbot_response(q_four)}')\n",
    "print(f'Question: {q_five}')\n",
    "print(f'Answer: {chatbot_response(q_five)}')\n",
    "print(f'Question: {q_six}')\n",
    "print(f'Answer: {chatbot_response(q_six)}')\n",
    "print(f'Question: {q_seven}')\n",
    "print(f'Answer: {chatbot_response(q_seven)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 3 - Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in data\n",
    "\n",
    "CORPUS_PATH = '/Users/dylanhayashi/Desktop/Northwestern/NU_MSDS/453 - Natural Language Processing/453.10 - Final Project/employee/employee.txt'\n",
    "f=open(CORPUS_PATH,'r',errors = 'ignore')\n",
    "raw=f.read()\n",
    "\n",
    "qa_pipeline = pipeline(\"question-answering\")\n",
    "\n",
    "context = raw\n",
    "\n",
    "print(f'Question: {q_one}')\n",
    "print(f\"Answer: {qa_pipeline(context=context, question=q_one)['answer']}\")\n",
    "print(f'Question: {q_two}')\n",
    "print(f\"Answer: {qa_pipeline(context=context, question=q_two)['answer']}\")\n",
    "print(f'Question: {q_three}')\n",
    "print(f\"Answer: {qa_pipeline(context=context, question=q_three)['answer']}\")\n",
    "print(f'Question: {q_four}')\n",
    "print(f\"Answer: {qa_pipeline(context=context, question=q_four)['answer']}\")\n",
    "print(f'Question: {q_five}')\n",
    "print(f\"Answer: {qa_pipeline(context=context, question=q_five)['answer']}\")\n",
    "print(f'Question: {q_six}')\n",
    "print(f\"Answer: {qa_pipeline(context=context, question=q_six)['answer']}\")\n",
    "print(f'Question: {q_seven}')\n",
    "print(f\"Answer: {qa_pipeline(context=context, question=q_seven)['answer']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
