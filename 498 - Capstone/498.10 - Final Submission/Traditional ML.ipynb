{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nyv7CG-y92Io"
      },
      "source": [
        "#### Machine Learning via SciKit Learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wvZst0fN92Iq"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import tensorflow as tf\n",
        "\n",
        "# Pre processing\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Feature Engineering\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Modeling\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "\n",
        "# Metrics\n",
        "from sklearn.metrics._classification import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Model Saving\n",
        "from joblib import dump\n",
        "from joblib import load"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yiREFtnI92Ir"
      },
      "source": [
        "#### Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q4cygV_292Ir"
      },
      "outputs": [],
      "source": [
        "def evaluate_models(true_labels, *model_predictions):\n",
        "    metrics = ['Accuracy', 'Precision', 'Recall', 'F1 Score', 'Specificity']\n",
        "\n",
        "    evaluation_results = []\n",
        "\n",
        "    for predictions in model_predictions:\n",
        "        accuracy = accuracy_score(true_labels, predictions)\n",
        "        precision = precision_score(true_labels, predictions, average='weighted', zero_division=0)\n",
        "        recall = recall_score(true_labels, predictions, average='weighted', zero_division=0)\n",
        "        f1 = f1_score(true_labels, predictions, average='weighted', zero_division=0)\n",
        "\n",
        "        # Calculate specificity\n",
        "        tn, fp, fn, tp = confusion_matrix(true_labels, predictions).ravel()\n",
        "        specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
        "\n",
        "        evaluation_results.append([accuracy, precision, recall, f1, specificity])\n",
        "\n",
        "    evaluation_df = pd.DataFrame(evaluation_results, columns=metrics)\n",
        "\n",
        "    return round(evaluation_df, 4)\n",
        "\n",
        "def plot_confusion_matrices(y_true,*model_predictions):\n",
        "\n",
        "    fig, axes = plt.subplots(3, 2, figsize=(18,20))\n",
        "    axes = axes.flatten()\n",
        "\n",
        "    for i, predictions in enumerate(model_predictions):\n",
        "        cm = confusion_matrix(y_true, predictions)\n",
        "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[i])\n",
        "        axes[i].set_title(f\"Model {i + 1} Confusion Matrix\")\n",
        "        axes[i].set_xlabel(\"Predicted\")\n",
        "        axes[i].set_ylabel(\"True\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def correct_distribution(images, labels, target_counts):    \n",
        "    \"\"\"\n",
        "    Resizes the dataset to have the specified number of instances for each class.\n",
        "\n",
        "    :param images: A numpy ndarray of shape (n, 299, 299, 1) representing the images.\n",
        "    :param labels: A numpy array of length n representing the labels.\n",
        "    :param target_counts: A dictionary where keys are labels and values are the target counts for each label.\n",
        "    :return: Resized images and labels arrays.\n",
        "    \"\"\"\n",
        "    resized_images = []\n",
        "    resized_labels = []\n",
        "\n",
        "    for label, count in target_counts.items():\n",
        "        # Find indices where the label matches\n",
        "        indices = np.where(labels == label)[0]\n",
        "\n",
        "        # If there are more instances than needed, randomly select 'count' instances\n",
        "        if len(indices) > count:\n",
        "            indices = np.random.choice(indices, count, replace=False)\n",
        "\n",
        "        # Append selected images and labels to the lists\n",
        "        resized_images.append(images[indices])\n",
        "        resized_labels.append(labels[indices])\n",
        "\n",
        "    # Concatenate all the selected images and labels\n",
        "    resized_images = np.concatenate(resized_images, axis=0)\n",
        "    resized_labels = np.concatenate(resized_labels, axis=0)\n",
        "\n",
        "    return resized_images, resized_labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Njm9gAtc92Ir"
      },
      "source": [
        "#### Import Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Dataset: CBIS-DDSM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MobO14c_92Ir"
      },
      "outputs": [],
      "source": [
        "# Import data in numpy objects\n",
        "cv10_data_filepath = '/Users/dylanhayashi/Desktop/Northwestern/498 - Capstone/CBIS-DDSM tf/cv10_data.npy'\n",
        "cv10_labels_filepath = '/Users/dylanhayashi/Desktop/Northwestern/498 - Capstone/CBIS-DDSM tf/cv10_labels.npy'\n",
        "test10_data_filepath = '/Users/dylanhayashi/Desktop/Northwestern/498 - Capstone/CBIS-DDSM tf/test10_data.npy'\n",
        "test10_labels_filepath = '/Users/dylanhayashi/Desktop/Northwestern/498 - Capstone/CBIS-DDSM tf/test10_labels.npy'\n",
        "cv10_data = np.load(cv10_data_filepath)\n",
        "cv10_labels = np.load(cv10_labels_filepath)\n",
        "test10_data = np.load(test10_data_filepath)\n",
        "test10_labels = np.load(test10_labels_filepath)\n",
        "\n",
        "# Import data from tfrecord datasets\n",
        "training_zero_filepath = '/Users/dylanhayashi/Desktop/Northwestern/498 - Capstone/CBIS-DDSM tf/training10_0.tfrecords'\n",
        "training_one_filepath = '/Users/dylanhayashi/Desktop/Northwestern/498 - Capstone/CBIS-DDSM tf/training10_1.tfrecords'\n",
        "training_two_filepath = '/Users/dylanhayashi/Desktop/Northwestern/498 - Capstone/CBIS-DDSM tf/training10_2.tfrecords'\n",
        "training_three_filepath = '/Users/dylanhayashi/Desktop/Northwestern/498 - Capstone/CBIS-DDSM tf/training10_3.tfrecords'\n",
        "training_four_filepath = '/Users/dylanhayashi/Desktop/Northwestern/498 - Capstone/CBIS-DDSM tf/training10_4.tfrecords'\n",
        "training_zero = tf.data.TFRecordDataset(training_zero_filepath)\n",
        "training_one = tf.data.TFRecordDataset(training_one_filepath)\n",
        "training_two = tf.data.TFRecordDataset(training_two_filepath)\n",
        "training_three = tf.data.TFRecordDataset(training_three_filepath)\n",
        "training_four = tf.data.TFRecordDataset(training_four_filepath)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Dataset: VinDr Mammo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def rescale_array(arr):\n",
        "    arr_min = arr.min()\n",
        "    arr_max = arr.max()\n",
        "    # Normalize the array to 0-1\n",
        "    normalized_arr = (arr - arr_min) / (arr_max - arr_min)\n",
        "    # Scale to 0-255\n",
        "    scaled_arr = normalized_arr * 255\n",
        "    return scaled_arr.astype(np.uint8)  # Convert to unsigned integer for image representation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "b_cal_images_filepath = '/Users/dylanhayashi/Desktop/Northwestern/498 - Capstone/VinDr-Mammo/images_unique_calcifications.npy'\n",
        "b_cal_labels_filepath = '/Users/dylanhayashi/Desktop/Northwestern/498 - Capstone/VinDr-Mammo/labels_unique_calcifications.npy'\n",
        "b_mass_images_filepath = '/Users/dylanhayashi/Desktop/Northwestern/498 - Capstone/VinDr-Mammo/images_unique_masses.npy'\n",
        "b_mass_labels_filepath = '/Users/dylanhayashi/Desktop/Northwestern/498 - Capstone/VinDr-Mammo/labels_unique_masses.npy'\n",
        "b_cal_images = np.load(b_cal_images_filepath)\n",
        "b_cal_labels = np.load(b_cal_labels_filepath)\n",
        "b_mass_images = np.load(b_mass_images_filepath)\n",
        "b_mass_labels = np.load(b_mass_labels_filepath)\n",
        "\n",
        "b_cal_images = rescale_array(b_cal_images)\n",
        "b_mass_images = rescale_array(b_mass_images)\n",
        "\n",
        "b_cal_labels[b_cal_labels == 1] = 3\n",
        "b_cal_labels[b_cal_labels == 0] = 1\n",
        "b_mass_labels[b_mass_labels == 0] = 2\n",
        "b_mass_labels[b_mass_labels == 1] = 4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "957Vmhbu92Is"
      },
      "source": [
        "#### Combine datasets into single dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wRUNi8HH92Is"
      },
      "outputs": [],
      "source": [
        "training_images = []\n",
        "training_labels = []\n",
        "\n",
        "feature_dictionary = {\n",
        "    'label':tf.io.FixedLenFeature([],tf.int64),\n",
        "    'label_normal':tf.io.FixedLenFeature([],tf.int64),\n",
        "    'image':tf.io.FixedLenFeature([],tf.string)\n",
        "}\n",
        "\n",
        "def _parse_function(example,feature_dictionary=feature_dictionary):\n",
        "  parsed_example = tf.io.parse_example(example,feature_dictionary)\n",
        "  return(parsed_example)\n",
        "\n",
        "def read_data(dataset):\n",
        "  read_dataset = dataset.map(_parse_function)\n",
        "  for features in read_dataset:\n",
        "      image = tf.io.decode_raw(features['image'], tf.uint8)\n",
        "      image = tf.reshape(image, [299, 299,1])\n",
        "      image=image.numpy()\n",
        "      training_images.append(image)\n",
        "      training_labels.append(features['label'].numpy())\n",
        "\n",
        "for training_dataset in [training_zero,training_one,training_two,training_three,training_four]:\n",
        "  read_data(training_dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Turn one of the following three cells from raw to python to determine which dataset(s) to use."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# CBIS-DDSM\n",
        "images = np.concatenate((training_images,cv10_data,test10_data),axis=0)\n",
        "labels = np.concatenate((training_labels,cv10_labels,test10_labels),axis=0)"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "# VinDr Mammo\n",
        "images = np.concatenate((b_cal_images,b_mass_images),axis=0)\n",
        "labels = np.concatenate((b_cal_labels,b_mass_labels),axis=0)"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "# CBID-DDSM + VinDr Mammo\n",
        "images = np.concatenate((training_images,cv10_data,test10_data,b_cal_images,b_mass_images),axis=0)\n",
        "labels = np.concatenate((training_labels,cv10_labels,test10_labels,b_cal_labels,b_mass_labels),axis=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Set dataset distribution according to dataset(s) choice and classification task"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2HQYsxMN92It"
      },
      "outputs": [],
      "source": [
        "target_counts = {0:9985, 1: 2768, 2: 2542, 3: 1849, 4: 2240}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "images, labels = correct_distribution(images, labels, target_counts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Relabel data according to classification task"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Task 1 - Negative vs Abnormality\n",
        "labels[(labels==2) | (labels==3) | (labels==4)] = 1\n",
        "\n",
        "# Task 2 - Calcification vs Mass\n",
        "# labels[(labels==1) | (labels==3)] = 0\n",
        "# labels[(labels==2) | (labels==4)] = 1\n",
        "\n",
        "# Task 3 - Benign vs Malignant Calcification\n",
        "# labels[labels==1] = 0\n",
        "# labels[labels==3] = 1\n",
        "\n",
        "# Task 4 - Benign vs Malignant Mass\n",
        "# labels[labels==2] = 0\n",
        "# labels[labels==4] = 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Create train and test splits, flatten, normalize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create train test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(images, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Flatten arrays for traditional ML algorithms\n",
        "train_n, height, width, channels = X_train.shape\n",
        "X_train_flat = X_train.reshape((train_n, height * width * channels))\n",
        "test_n, height, width, channels = X_test.shape\n",
        "X_test_flat = X_test.reshape((test_n, height * width * channels))\n",
        "\n",
        "# Create normalized versions\n",
        "scaler = MinMaxScaler()\n",
        "scaler.fit(X_train_flat)\n",
        "X_train_norm = scaler.transform(X_train_flat)\n",
        "X_test_norm = scaler.transform(X_test_flat)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Modeling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_one = GaussianNB()\n",
        "model_one.fit(X_train_flat,y_train)\n",
        "pred_one = model_one.predict(X_test_flat)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_two = LogisticRegression()\n",
        "model_two.fit(X_train_norm,y_train)\n",
        "pred_two = model_two.predict(X_test_norm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_three = SVC()\n",
        "model_three.fit(X_train_norm,y_train)\n",
        "pred_three = model_three.predict(X_test_norm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_five = RandomForestClassifier()\n",
        "model_five.fit(X_train_norm,y_train)\n",
        "pred_five = model_five.predict(X_test_flat)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_six = DecisionTreeClassifier()\n",
        "model_six.fit(X_train_norm,y_train)\n",
        "pred_six = model_six.predict(X_test_flat)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_seven = RandomForestClassifier()\n",
        "model_seven.fit(X_train_norm,y_train)\n",
        "pred_seven = model_seven.predict(X_test_flat)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_dict = {\n",
        "    'Model One':'Naive Bayes',\n",
        "    'Model Two':'Logistic Regression',\n",
        "    'Model Three':'Support Vector Machine',\n",
        "    'Model Four':'K-Nearest Neighbors',\n",
        "    'Model Five':'Decision Tree',\n",
        "    'Model Six':'Random Forest',\n",
        "    'Model Seven':'XGBoost'\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "eval = evaluate_models(y_test,pred_one,pred_two,pred_three,pred_five)\n",
        "eval['Model'] = [key for key in model_dict.keys()]\n",
        "eval['Model Type'] = [values[0] for values in model_dict.values()]\n",
        "eval.round(2)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.6 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
