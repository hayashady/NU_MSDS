{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from packaging import version\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import seaborn as sns\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import tensorflow_datasets as tfds\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow.keras.backend as k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualization Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_validation_report(test_labels, predictions):\n",
    "    print(\"Classification Report\")\n",
    "    print(classification_report(test_labels, predictions))\n",
    "    print('Accuracy Score: {}'.format(accuracy_score(test_labels, predictions)))\n",
    "    print('Root Mean Square Error: {}'.format(np.sqrt(MSE(test_labels, predictions))))\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred):\n",
    "    mtx = confusion_matrix(y_true, y_pred)\n",
    "    fig, ax = plt.subplots(figsize=(8,8))\n",
    "    sns.heatmap(mtx, annot=True, fmt='d', linewidths=.75,  cbar=False, ax=ax,cmap='Blues',linecolor='white')\n",
    "    #  square=True,\n",
    "    plt.ylabel('true label')\n",
    "    plt.xlabel('predicted label')\n",
    "\n",
    "def plot_graphs(history, metric):\n",
    "  plt.plot(history.history[metric])\n",
    "  plt.plot(history.history['val_'+metric], '')\n",
    "  plt.xlabel(\"Epochs\")\n",
    "  plt.ylabel(metric)\n",
    "  plt.legend([metric, 'val_'+metric])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/bin/python: Error while finding module specification for 'tensorflow_datasets.scripts.download_and_prepare' (ModuleNotFoundError: No module named 'tensorflow_datasets')\n"
     ]
    }
   ],
   "source": [
    "# register  ag_news_subset so that tfds.load doesn't generate a checksum (mismatch) error\n",
    "!python -m tensorflow_datasets.scripts.download_and_prepare --register_checksums --datasets=ag_news_subset\n",
    "\n",
    "dataset,info=\\\n",
    "tfds.load('ag_news_subset', with_info=True,  split=['train[:95%]','train[95%:]', 'test'],batch_size = 32\n",
    "          , as_supervised=True)\n",
    "\n",
    "train_ds, val_ds, test_ds = dataset\n",
    "text_only_train_ds = train_ds.map(lambda x, y: x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords',quiet=True)\n",
    "STOPWORDS = stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_stopwords(input_text):\n",
    "    lowercase = tf.strings.lower(input_text)\n",
    "    stripped_punct = tf.strings.regex_replace(lowercase\n",
    "                                  ,'[%s]' % re.escape(string.punctuation)\n",
    "                                  ,'')\n",
    "    return tf.strings.regex_replace(stripped_punct, r'\\b(' + r'|'.join(STOPWORDS) + r')\\b\\s*',\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 96\n",
    "max_tokens = 1000\n",
    "text_vectorization = layers.TextVectorization(\n",
    "    max_tokens=max_tokens,\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=max_length,\n",
    "    standardize=custom_stopwords\n",
    ")\n",
    "text_vectorization.adapt(text_only_train_ds)\n",
    "\n",
    "int_train_ds = train_ds.map(\n",
    "    lambda x, y: (text_vectorization(x), y),\n",
    "    num_parallel_calls=4)\n",
    "int_val_ds = val_ds.map(\n",
    "    lambda x, y: (text_vectorization(x), y),\n",
    "    num_parallel_calls=4)\n",
    "int_test_ds = test_ds.map(\n",
    "    lambda x, y: (text_vectorization(x), y),\n",
    "    num_parallel_calls=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Experiment 1 - 1D CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output_sequence_length=96 # CONSTANT\n",
    "dropout = 0.2 # CONSTANT\n",
    "kernel_size = 3 # VARIABLE\n",
    "filters = 32 # CONSTANT\n",
    "input_dim = 1000 # CONSTANT, embedded one-hot encoding depth\n",
    "units = 256 # CONSTANT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "3563/3563 [==============================] - 32s 9ms/step - loss: 0.5298 - accuracy: 0.8086 - val_loss: 0.4226 - val_accuracy: 0.8528\n",
      "Epoch 2/200\n",
      "3563/3563 [==============================] - 32s 9ms/step - loss: 0.4304 - accuracy: 0.8478 - val_loss: 0.4163 - val_accuracy: 0.8587\n",
      "Epoch 3/200\n",
      "3563/3563 [==============================] - 33s 9ms/step - loss: 0.4228 - accuracy: 0.8515 - val_loss: 0.4167 - val_accuracy: 0.8600\n",
      "Epoch 4/200\n",
      "3563/3563 [==============================] - 32s 9ms/step - loss: 0.4208 - accuracy: 0.8535 - val_loss: 0.4185 - val_accuracy: 0.8592\n",
      "Epoch 5/200\n",
      "3563/3563 [==============================] - 31s 9ms/step - loss: 0.4202 - accuracy: 0.8541 - val_loss: 0.4259 - val_accuracy: 0.8557\n",
      "Epoch 6/200\n",
      "3563/3563 [==============================] - 33s 9ms/step - loss: 0.4243 - accuracy: 0.8536 - val_loss: 0.4285 - val_accuracy: 0.8578\n",
      "238/238 [==============================] - 1s 4ms/step - loss: 0.4287 - accuracy: 0.8488\n",
      "Test acc: 0.849\n",
      "238/238 [==============================] - 1s 4ms/step\n"
     ]
    }
   ],
   "source": [
    "k.clear_session()\n",
    "\n",
    "inputs = tf.keras.Input(shape=(None,), dtype=\"int64\")\n",
    "\n",
    "embedded = tf.one_hot(inputs, depth=input_dim)\n",
    "\n",
    "x = layers.Conv1D(filters=filters, kernel_size=kernel_size, activation='relu')(embedded)\n",
    "x = layers.Dropout(dropout)(x)\n",
    "x = layers.MaxPooling1D(pool_size=2)(x)\n",
    "x = layers.GlobalMaxPooling1D()(x)\n",
    "x = layers.Dense(units, activation='relu')(x)\n",
    "\n",
    "outputs = layers.Dense(4, activation=\"softmax\")(x)\n",
    "\n",
    "model = tf.keras.Model(inputs, outputs)\n",
    "\n",
    "model.compile(optimizer=\"rmsprop\",\n",
    "              loss=\"SparseCategoricalCrossentropy\",\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.ModelCheckpoint(\"LSTM.h5\",save_best_only=True)\n",
    "    ,tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=3)\n",
    "]\n",
    "\n",
    "history=model.fit(int_train_ds, validation_data=int_val_ds, epochs=200, callbacks=callbacks)\n",
    "\n",
    "model = keras.models.load_model(\"LSTM.h5\")\n",
    "\n",
    "print(f\"Test acc: {model.evaluate(int_test_ds)[1]:.3f}\")\n",
    "\n",
    "y_test = np.concatenate([y for x, y in int_test_ds], axis=0)\n",
    "\n",
    "pred_classes = np.argmax(model.predict(int_test_ds), axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Experiment 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Experiment 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Experiment 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Experiment 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Experiment 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Experiment 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Experiment 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Experiment 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Experiment 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "k.clear_session()\n",
    "inputs = tf.keras.Input(shape=(None,), dtype=\"int64\")\n",
    "embedded = layers.Embedding(input_dim=max_tokens\n",
    "                            ,output_dim=256\n",
    "                            ,mask_zero=True)(inputs)\n",
    "x = layers.Bidirectional(layers.LSTM(32))(embedded)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "outputs = layers.Dense(4, activation=\"softmax\")(x)\n",
    "model = tf.keras.Model(inputs, outputs)\n",
    "model.compile(optimizer=\"rmsprop\",\n",
    "              loss=\"SparseCategoricalCrossentropy\",\n",
    "              metrics=[\"accuracy\"])\n",
    "model.summary()\n",
    "\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.ModelCheckpoint(\"LSTM.h5\",save_best_only=True)\n",
    "    ,tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=3)\n",
    "]\n",
    "history=model.fit(int_train_ds, validation_data=int_val_ds, epochs=200, callbacks=callbacks)\n",
    "model = keras.models.load_model(\"LSTM.h5\")\n",
    "print(f\"Test acc: {model.evaluate(int_test_ds)[1]:.3f}\")\n",
    "\n",
    "y_test = np.concatenate([y for x, y in int_test_ds], axis=0)\n",
    "pred_classes = np.argmax(model.predict(int_test_ds), axis=-1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
