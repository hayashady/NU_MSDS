{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nyv7CG-y92Io"
      },
      "source": [
        "#### Transfer Learning via Keras Tensorflow Applications API"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wvZst0fN92Iq"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import tensorflow as tf\n",
        "\n",
        "# Pre processing\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Feature Engineering\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Modeling\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "from tensorflow.keras.applications import ResNet50, ResNet50V2, ResNet101, ResNet101V2, ResNet152, ResNet152V2, InceptionV3, DenseNet121, Xception, EfficientNetV2B0\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Flatten\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "# Metrics\n",
        "from sklearn.metrics._classification import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Model Saving\n",
        "from joblib import dump\n",
        "from joblib import load"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yiREFtnI92Ir"
      },
      "source": [
        "#### Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q4cygV_292Ir"
      },
      "outputs": [],
      "source": [
        "def evaluate_models(true_labels, *model_predictions):\n",
        "    metrics = ['Accuracy', 'Precision', 'Recall', 'F1 Score', 'Specificity']\n",
        "\n",
        "    evaluation_results = []\n",
        "\n",
        "    for predictions in model_predictions:\n",
        "        accuracy = accuracy_score(true_labels, predictions)\n",
        "        precision = precision_score(true_labels, predictions, average='weighted', zero_division=0)\n",
        "        recall = recall_score(true_labels, predictions, average='weighted', zero_division=0)\n",
        "        f1 = f1_score(true_labels, predictions, average='weighted', zero_division=0)\n",
        "\n",
        "        # Calculate specificity\n",
        "        tn, fp, fn, tp = confusion_matrix(true_labels, predictions).ravel()\n",
        "        specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
        "\n",
        "        evaluation_results.append([accuracy, precision, recall, f1, specificity])\n",
        "\n",
        "    evaluation_df = pd.DataFrame(evaluation_results, columns=metrics)\n",
        "\n",
        "    return round(evaluation_df, 4)\n",
        "\n",
        "def plot_confusion_matrices(y_true,*model_predictions):\n",
        "\n",
        "    fig, axes = plt.subplots(3, 2, figsize=(18,20))\n",
        "    axes = axes.flatten()\n",
        "\n",
        "    for i, predictions in enumerate(model_predictions):\n",
        "        cm = confusion_matrix(y_true, predictions)\n",
        "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[i])\n",
        "        axes[i].set_title(f\"Model {i + 1} Confusion Matrix\")\n",
        "        axes[i].set_xlabel(\"Predicted\")\n",
        "        axes[i].set_ylabel(\"True\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def correct_distribution(images, labels, target_counts):    \n",
        "    \"\"\"\n",
        "    Resizes the dataset to have the specified number of instances for each class.\n",
        "\n",
        "    :param images: A numpy ndarray of shape (n, 299, 299, 1) representing the images.\n",
        "    :param labels: A numpy array of length n representing the labels.\n",
        "    :param target_counts: A dictionary where keys are labels and values are the target counts for each label.\n",
        "    :return: Resized images and labels arrays.\n",
        "    \"\"\"\n",
        "    resized_images = []\n",
        "    resized_labels = []\n",
        "\n",
        "    for label, count in target_counts.items():\n",
        "        # Find indices where the label matches\n",
        "        indices = np.where(labels == label)[0]\n",
        "\n",
        "        # If there are more instances than needed, randomly select 'count' instances\n",
        "        if len(indices) > count:\n",
        "            indices = np.random.choice(indices, count, replace=False)\n",
        "\n",
        "        # Append selected images and labels to the lists\n",
        "        resized_images.append(images[indices])\n",
        "        resized_labels.append(labels[indices])\n",
        "\n",
        "    # Concatenate all the selected images and labels\n",
        "    resized_images = np.concatenate(resized_images, axis=0)\n",
        "    resized_labels = np.concatenate(resized_labels, axis=0)\n",
        "\n",
        "    return resized_images, resized_labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Njm9gAtc92Ir"
      },
      "source": [
        "#### Import Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Dataset: CBIS-DDSM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MobO14c_92Ir"
      },
      "outputs": [],
      "source": [
        "# Import data in numpy objects\n",
        "cv10_data_filepath = '/Users/dylanhayashi/Desktop/Northwestern/498 - Capstone/CBIS-DDSM tf/cv10_data.npy'\n",
        "cv10_labels_filepath = '/Users/dylanhayashi/Desktop/Northwestern/498 - Capstone/CBIS-DDSM tf/cv10_labels.npy'\n",
        "test10_data_filepath = '/Users/dylanhayashi/Desktop/Northwestern/498 - Capstone/CBIS-DDSM tf/test10_data.npy'\n",
        "test10_labels_filepath = '/Users/dylanhayashi/Desktop/Northwestern/498 - Capstone/CBIS-DDSM tf/test10_labels.npy'\n",
        "cv10_data = np.load(cv10_data_filepath)\n",
        "cv10_labels = np.load(cv10_labels_filepath)\n",
        "test10_data = np.load(test10_data_filepath)\n",
        "test10_labels = np.load(test10_labels_filepath)\n",
        "\n",
        "# Import data from tfrecord datasets\n",
        "training_zero_filepath = '/Users/dylanhayashi/Desktop/Northwestern/498 - Capstone/CBIS-DDSM tf/training10_0.tfrecords'\n",
        "training_one_filepath = '/Users/dylanhayashi/Desktop/Northwestern/498 - Capstone/CBIS-DDSM tf/training10_1.tfrecords'\n",
        "training_two_filepath = '/Users/dylanhayashi/Desktop/Northwestern/498 - Capstone/CBIS-DDSM tf/training10_2.tfrecords'\n",
        "training_three_filepath = '/Users/dylanhayashi/Desktop/Northwestern/498 - Capstone/CBIS-DDSM tf/training10_3.tfrecords'\n",
        "training_four_filepath = '/Users/dylanhayashi/Desktop/Northwestern/498 - Capstone/CBIS-DDSM tf/training10_4.tfrecords'\n",
        "training_zero = tf.data.TFRecordDataset(training_zero_filepath)\n",
        "training_one = tf.data.TFRecordDataset(training_one_filepath)\n",
        "training_two = tf.data.TFRecordDataset(training_two_filepath)\n",
        "training_three = tf.data.TFRecordDataset(training_three_filepath)\n",
        "training_four = tf.data.TFRecordDataset(training_four_filepath)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Dataset: VinDr Mammo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def rescale_array(arr):\n",
        "    arr_min = arr.min()\n",
        "    arr_max = arr.max()\n",
        "    # Normalize the array to 0-1\n",
        "    normalized_arr = (arr - arr_min) / (arr_max - arr_min)\n",
        "    # Scale to 0-255\n",
        "    scaled_arr = normalized_arr * 255\n",
        "    return scaled_arr.astype(np.uint8)  # Convert to unsigned integer for image representation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "b_cal_images_filepath = '/Users/dylanhayashi/Desktop/Northwestern/498 - Capstone/VinDr-Mammo/images_unique_calcifications.npy'\n",
        "b_cal_labels_filepath = '/Users/dylanhayashi/Desktop/Northwestern/498 - Capstone/VinDr-Mammo/labels_unique_calcifications.npy'\n",
        "b_mass_images_filepath = '/Users/dylanhayashi/Desktop/Northwestern/498 - Capstone/VinDr-Mammo/images_unique_masses.npy'\n",
        "b_mass_labels_filepath = '/Users/dylanhayashi/Desktop/Northwestern/498 - Capstone/VinDr-Mammo/labels_unique_masses.npy'\n",
        "b_cal_images = np.load(b_cal_images_filepath)\n",
        "b_cal_labels = np.load(b_cal_labels_filepath)\n",
        "b_mass_images = np.load(b_mass_images_filepath)\n",
        "b_mass_labels = np.load(b_mass_labels_filepath)\n",
        "\n",
        "b_cal_images = rescale_array(b_cal_images)\n",
        "b_mass_images = rescale_array(b_mass_images)\n",
        "\n",
        "b_cal_labels[b_cal_labels == 1] = 3\n",
        "b_cal_labels[b_cal_labels == 0] = 1\n",
        "b_mass_labels[b_mass_labels == 0] = 2\n",
        "b_mass_labels[b_mass_labels == 1] = 4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "957Vmhbu92Is"
      },
      "source": [
        "#### Combine training, validation, and test sets into single dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wRUNi8HH92Is"
      },
      "outputs": [],
      "source": [
        "training_images = []\n",
        "training_labels = []\n",
        "\n",
        "feature_dictionary = {\n",
        "    'label':tf.io.FixedLenFeature([],tf.int64),\n",
        "    'label_normal':tf.io.FixedLenFeature([],tf.int64),\n",
        "    'image':tf.io.FixedLenFeature([],tf.string)\n",
        "}\n",
        "\n",
        "def _parse_function(example,feature_dictionary=feature_dictionary):\n",
        "  parsed_example = tf.io.parse_example(example,feature_dictionary)\n",
        "  return(parsed_example)\n",
        "\n",
        "def read_data(dataset):\n",
        "  read_dataset = dataset.map(_parse_function)\n",
        "  for features in read_dataset:\n",
        "      image = tf.io.decode_raw(features['image'], tf.uint8)\n",
        "      image = tf.reshape(image, [299, 299,1])\n",
        "      image=image.numpy()\n",
        "      training_images.append(image)\n",
        "      training_labels.append(features['label'].numpy())\n",
        "\n",
        "for training_dataset in [training_zero,training_one,training_two,training_three,training_four]:\n",
        "  read_data(training_dataset)\n",
        "\n",
        "  # Create arrays of images and labels\n",
        "images = np.concatenate((training_images,cv10_data,test10_data,b_cal_images,b_mass_images),axis=0)\n",
        "labels = np.concatenate((training_labels,cv10_labels,test10_labels,b_cal_labels,b_mass_labels),axis=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Turn one of the following three cells from raw to python to determine which dataset(s) to use."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# CBIS-DDSM\n",
        "images = np.concatenate((training_images,cv10_data,test10_data),axis=0)\n",
        "labels = np.concatenate((training_labels,cv10_labels,test10_labels),axis=0)"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "# VinDr Mammo\n",
        "images = np.concatenate((b_cal_images,b_mass_images),axis=0)\n",
        "labels = np.concatenate((b_cal_labels,b_mass_labels),axis=0)"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "# CBID-DDSM + VinDr Mammo\n",
        "images = np.concatenate((training_images,cv10_data,test10_data,b_cal_images,b_mass_images),axis=0)\n",
        "labels = np.concatenate((training_labels,cv10_labels,test10_labels,b_cal_labels,b_mass_labels),axis=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Set dataset distribution according to dataset(s) choice and classification task"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "target_counts = {0:9985, 1: 2768, 2: 2542, 3: 1849, 4: 2240}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "images, labels = correct_distribution(images, labels, target_counts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Relabel data according to classification task"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Task 1 - Negative vs Abnormality\n",
        "labels[(labels==2) | (labels==3) | (labels==4)] = 1\n",
        "\n",
        "# Task 2 - Calcification vs Mass\n",
        "# labels[(labels==1) | (labels==3)] = 0\n",
        "# labels[(labels==2) | (labels==4)] = 1\n",
        "\n",
        "# Task 3 - Benign vs Malignant Calcification\n",
        "# labels[labels==1] = 0\n",
        "# labels[labels==3] = 1\n",
        "\n",
        "# Task 4 - Benign vs Malignant Mass\n",
        "# labels[labels==2] = 0\n",
        "# labels[labels==4] = 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create deep learning versions\n",
        "X_train, X_test, y_train, y_test = train_test_split(images, labels, test_size=0.2, random_state=42)\n",
        "X_train_dl, X_valid_dl, y_train_dl, y_valid_dl = train_test_split(X_train, y_train,train_size=0.8,test_size=0.2)\n",
        "X_train_dl = X_train_dl.astype('float32') / 255\n",
        "X_valid_dl = X_valid_dl.astype('float32') / 255\n",
        "X_test_dl = X_test.astype('float32') / 255\n",
        "\n",
        "X_train_3ch = np.repeat(X_train_dl, 3, axis=-1)\n",
        "X_valid_3ch = np.repeat(X_valid_dl, 3, axis=-1)\n",
        "X_test_3ch = np.repeat(X_test_dl, 3, axis=-1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Modeling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "base_model = ResNet101V2(weights='imagenet', include_top=False, input_shape=(299, 299, 3))\n",
        "base_model.trainable = False\n",
        "x = base_model.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "x = Dense(64, activation='relu')(x) \n",
        "predictions = Dense(2, activation='softmax')(x) \n",
        "model = Model(inputs=base_model.input, outputs=predictions)\n",
        "callbacks = [tf.keras.callbacks.ModelCheckpoint(\"model.h5\", save_best_only=True),tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=5)]\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "model.fit(X_train_3ch, y_train_dl, epochs=20, validation_data=(X_valid_3ch, y_valid_dl), callbacks=callbacks, verbose=False)\n",
        "#\n",
        "model = keras.models.load_model(\"model.h5\")\n",
        "pred = np.argmax(model.predict(X_test_3ch,verbose=False),axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_list = ['ResNet101V2']\n",
        "\n",
        "eval = evaluate_models(\n",
        "    y_test,pred,\n",
        ")\n",
        "eval['Model'] = model_list\n",
        "eval = eval.loc[:,['Model','Accuracy','Precision','Recall','F1 Score','Specificity','Training Time']]\n",
        "eval.loc[:,['Accuracy','Precision','Recall','F1 Score','Specificity']].round(2)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.6 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
