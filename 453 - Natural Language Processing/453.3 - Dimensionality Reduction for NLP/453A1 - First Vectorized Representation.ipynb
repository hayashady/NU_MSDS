{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import string\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import random\n",
    "from dataclasses import dataclass\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.manifold import TSNE\n",
    "import scipy.cluster.hierarchy\n",
    "from IPython.display import display, HTML\n",
    "from typing import List, Callable, Dict\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assignment 1 Functions\n",
    "def add_movie_descriptor(data: pd.DataFrame, corpus_df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Adds \"Movie Description\" to the supplied dataframe, in the form {Genre}_{P|N}_{Movie Title}_{DocID}\n",
    "    \"\"\"\n",
    "    review = np.where(corpus_df['Review Type (pos or neg)'] == 'Positive', 'P', 'N')\n",
    "    data['Descriptor'] = corpus_df['Genre of Movie'] + '_' + corpus_df['Movie Title'] + '_' + review + '_' + corpus_df['Doc_ID'].astype(str)\n",
    "\n",
    "def get_corpus_df(path):\n",
    "    data = pd.read_csv(path, encoding=\"utf-8\")\n",
    "    add_movie_descriptor(data, data)\n",
    "    sorted_data = data.sort_values(['Descriptor'])\n",
    "    indexed_data = sorted_data.set_index(['Doc_ID'])\n",
    "    indexed_data['Doc_ID'] = indexed_data.index\n",
    "    return indexed_data\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    return re.sub('[^a-zA-Z]', ' ', str(text))\n",
    "\n",
    "def lower_case(text):\n",
    "    return text.lower()    \n",
    "\n",
    "def remove_tags(text):    \n",
    "    return re.sub(\"&lt;/?.*?&gt;\",\" &lt;&gt; \", text)\n",
    "\n",
    "def remove_special_chars_and_digits(text):\n",
    "    return re.sub(\"(\\\\d|\\\\W)+\",\" \", text)\n",
    "\n",
    "@dataclass\n",
    "class Document:\n",
    "    doc_id: str\n",
    "    text: str\n",
    "\n",
    "def normalize_document(document: Document) -> Document:\n",
    "    text = document.text\n",
    "    text = remove_punctuation(text)\n",
    "    text = lower_case(text)\n",
    "    text = remove_tags(text)\n",
    "    text = remove_special_chars_and_digits(text)\n",
    "    \n",
    "    return Document(document.doc_id, text)\n",
    "\n",
    "def normalize_documents(documents: List[Document]) -> List[Document]:\n",
    "    \"\"\"\n",
    "    Normalizes text for all given documents.\n",
    "    Removes punctuation, converts to lower case, removes tags and special characters.\n",
    "    \"\"\"\n",
    "    return [normalize_document(x) for x in documents]\n",
    "\n",
    "@dataclass\n",
    "class TokenizedDocument:\n",
    "    doc_id: str\n",
    "    tokens: List[str]\n",
    "\n",
    "def tokenize_document(document: Document) -> TokenizedDocument:\n",
    "    tokens = nltk.word_tokenize(document.text)\n",
    "    return TokenizedDocument(document.doc_id, tokens)\n",
    "\n",
    "def tokenize_documents(documents: List[Document]) -> List[TokenizedDocument]:\n",
    "    return [tokenize_document(x) for x in documents]\n",
    "\n",
    "def lemmatize(documents: List[TokenizedDocument]) -> List[TokenizedDocument]:\n",
    "    result = []\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    for document in documents:\n",
    "        output_tokens = [lemmatizer.lemmatize(w) for w in document.tokens]\n",
    "        result.append(TokenizedDocument(document.doc_id, output_tokens))\n",
    "        \n",
    "    return result\n",
    "\n",
    "def stem(documents: List[TokenizedDocument]) -> List[TokenizedDocument]:\n",
    "    result = []\n",
    "    stemmer = PorterStemmer()\n",
    "    for document in documents:\n",
    "        output_tokens = [stemmer.stem(w) for w in document.tokens]\n",
    "        result.append(TokenizedDocument(document.doc_id, output_tokens))\n",
    "\n",
    "    return result\n",
    "\n",
    "def remove_stop_words(documents: List[TokenizedDocument]) -> List[TokenizedDocument]:\n",
    "    result = []\n",
    "    \n",
    "    stop_words = set(nltk.corpus.stopwords.words('english')) \n",
    "    for document in documents:\n",
    "        filtered_tokens = [w for w in document.tokens if not w in stop_words]\n",
    "        result.append(TokenizedDocument(document.doc_id, filtered_tokens))\n",
    "        \n",
    "    return result\n",
    "    \n",
    "def add_flags(data: pd.DataFrame, casino_royale_doc_ids: List[int], action_doc_ids: List[int]):\n",
    "    data['is_casino_royale'] = data.index.isin(casino_royale_doc_ids)\n",
    "    data['is_action'] = data.index.isin(action_doc_ids)\n",
    "    \n",
    "def get_all_tokens(documents: List[TokenizedDocument]) -> List[str]:\n",
    "    tokens = {y for x in documents for y in x.tokens}\n",
    "    return sorted(list(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizing Text Functions\n",
    "def add_movie_descriptor(data: pd.DataFrame, corpus_df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Adds \"Movie Description\" to the supplied dataframe, in the form {Genre}_{P|N}_{Movie Title}_{DocID}\n",
    "    \"\"\"\n",
    "    review = np.where(corpus_df['Review Type (pos or neg)'] == 'Positive', 'P', 'N')\n",
    "    data['Descriptor'] = corpus_df['Genre of Movie'] + '_' + corpus_df['Movie Title'] + '_' + review + '_' + corpus_df['Doc_ID'].astype(str)\n",
    "\n",
    "def get_corpus_df(path):\n",
    "    data = pd.read_csv(path, encoding=\"utf-8\")\n",
    "    add_movie_descriptor(data, data)\n",
    "    sorted_data = data.sort_values(['Descriptor'])\n",
    "    indexed_data = sorted_data.set_index(['Doc_ID'])\n",
    "    indexed_data['Doc_ID'] = indexed_data.index\n",
    "    return indexed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorizing Tokens\n",
    "def add_movie_descriptor(data: pd.DataFrame, corpus_df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Adds \"Movie Description\" to the supplied dataframe, in the form {Genre}_{P|N}_{Movie Title}_{DocID}\n",
    "    \"\"\"\n",
    "    review = np.where(corpus_df['Review Type (pos or neg)'] == 'Positive', 'P', 'N')\n",
    "    data['Descriptor'] = corpus_df['Genre of Movie'] + '_' + corpus_df['Movie Title'] + '_' + review + '_' + corpus_df['Doc_ID'].astype(str)\n",
    "\n",
    "def get_corpus_df(path):\n",
    "    data = pd.read_csv(path, encoding=\"utf-8\")\n",
    "    add_movie_descriptor(data, data)\n",
    "    sorted_data = data.sort_values(['Descriptor'])\n",
    "    indexed_data = sorted_data.set_index(['Doc_ID'])\n",
    "    indexed_data['Doc_ID'] = indexed_data.index\n",
    "    return indexed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Data Wrangling (tokenization, stop words, lemmatization, stemming, punctuation, lower case)\n",
    "# 2. Text Vectorization using tf-ids\n",
    "# 3. Text Vectorization using Dov2Vec\n",
    "# 4. Text Vectorization using Word2Vec "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
